{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b39e6d19-7e3e-4bce-aa42-b38c230bbeb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/homebrew/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/maksim.nikiforov/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/maksim.nikiforov/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-43beeeef-5a80-45cc-9f83-567454b66884;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 330ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-43beeeef-5a80-45cc-9f83-567454b66884\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 08:54:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Establish session\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b5e588-f296-4c74-aa52-0a7dffc2df43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable CDC globally (if you choose - you can also enable on a table-by-table basis, see https://docs.delta.io/2.0.0/delta-change-data-feed.html#enable-change-data-feed)\n",
    "spark.sql(\"set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24262ac7-cd8f-42b6-be39-4275917a721e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 08:57:40 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create initial table in local directory (replace path with yours)\n",
    "spark.sql(\"CREATE TABLE changing_addresses ( \\\n",
    "  primaryKey int, \\\n",
    "  address string, \\\n",
    "  current boolean, \\\n",
    "  effectiveDate string, \\\n",
    "  endDate string \\\n",
    "  ) \\\n",
    "USING delta \\\n",
    "LOCATION '/Users/maksim.nikiforov/Documents/cdc_tests/' \\\n",
    "TBLPROPERTIES (delta.enableChangeDataFeed = true)\" \\\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "903fa78d-8b92-49da-a144-fb57d7225050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populate changing_addresses Delta table with sample data\n",
    "spark.sql(\"INSERT INTO changing_addresses \\\n",
    "SELECT 11 primaryKey, 'A new customer address' as address, true as current, '2023-04-20' as effectiveDate, null as endDate \\\n",
    "UNION \\\n",
    "SELECT 41 primaryKey, 'A different address' as address, true as current, '2023-04-20' as effectiveDate, null as endDate \\\n",
    "UNION \\\n",
    "SELECT 58 primaryKey, 'Yet another address' as address, true as current, '2023-04-20' as effectiveDate, null as endDate\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03ccc13a-3980-49f8-998d-cef3a502348c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a temporary view with information we will merge later\n",
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW updates \\\n",
    "AS \\\n",
    "SELECT 11 primaryKey, 'An updated address' as address, true as current, '2020-10-22' as effectiveDate, null as endDate \\\n",
    "UNION \\\n",
    "SELECT 99 primaryKey, 'A completely new address' as address, true as current, '2020-10-22' as effectiveDate, null as endDate\" \\\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6cfca85-8f0c-4aab-8161-fa2e3e2cc569",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 09:02:14 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/20 09:02:15 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/20 09:02:15 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the information in our view with our Delta table\n",
    "spark.sql(\"MERGE INTO changing_addresses as original \\\n",
    "USING ( \\\n",
    "  SELECT updates.primaryKey as merge, updates.* \\\n",
    "  FROM updates \\\n",
    "  UNION ALL \\\n",
    "  SELECT null as merge, updates.* \\\n",
    "  FROM updates INNER JOIN \\\n",
    "  changing_addresses original on updates.primaryKey = original.primaryKey \\\n",
    "  WHERE original.current = true \\\n",
    ") mergeupdates \\\n",
    "ON original.primaryKey = mergeupdates.merge \\\n",
    "WHEN MATCHED and original.current = true THEN \\\n",
    "UPDATE SET current = false, endDate = mergeupdates.effectiveDate \\\n",
    "WHEN NOT MATCHED then \\\n",
    "INSERT *\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97d54bb4-3cb4-4775-b835-8cecfe6ecfc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a local table in Parquet\n",
    "# You do not have to enable CDC see table below for example without CDC\n",
    "spark.sql(\"CREATE TABLE mnik_address_updates ( \\\n",
    "  primaryKey int, \\\n",
    "  address string, \\\n",
    "  current boolean, \\\n",
    "  effectiveDate string, \\\n",
    "  endDate string \\\n",
    "  ) \\\n",
    "USING parquet \\\n",
    "LOCATION '/Users/maksim.nikiforov/Documents/address_updates/' \\\n",
    "TBLPROPERTIES (delta.enableChangeDataFeed = true)\" \\\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "509fc4f3-9f69-4ef1-8276-42b0a3bf79a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of Parquet table with no CDC\n",
    "spark.sql(\"CREATE TABLE some_address_updates ( \\\n",
    "  primaryKey int, \\\n",
    "  address string, \\\n",
    "  current boolean, \\\n",
    "  effectiveDate string, \\\n",
    "  endDate string \\\n",
    "  ) \\\n",
    "USING parquet \\\n",
    "LOCATION '/Users/maksim.nikiforov/Documents/my_address_updates/'\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d48d54d7-7769-4138-b4bf-3bc89e74e07b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Populate Parquet table with some information\n",
    "spark.sql(\"INSERT INTO some_address_updates \\\n",
    "SELECT 101 primaryKey, 'Yet another customer address' as address, true as current, '2018-01-02' as effectiveDate, null as endDate \\\n",
    "UNION \\\n",
    "SELECT 102 primaryKey, 'One more address' as address, true as current, '2018-01-01' as effectiveDate, null as endDate\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "991c9c0b-6fa5-4c4b-956c-4695dbccc735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 09:15:00 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/20 09:15:00 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge local Delta table with local Parquet-only table\n",
    "spark.sql(\"MERGE INTO changing_addresses as original \\\n",
    "USING ( \\\n",
    "  SELECT mnik_address_updates.primaryKey as merge, mnik_address_updates.* \\\n",
    "  FROM mnik_address_updates \\\n",
    "  UNION ALL \\\n",
    "  SELECT null as merge, mnik_address_updates.* \\\n",
    "  FROM mnik_address_updates INNER JOIN \\\n",
    "  changing_addresses original on mnik_address_updates.primaryKey = original.primaryKey \\\n",
    "  WHERE original.current = true \\\n",
    ") mergeupdates \\\n",
    "ON original.primaryKey = mergeupdates.merge \\\n",
    "WHEN MATCHED and original.current = true THEN \\\n",
    "UPDATE SET current = false, endDate = mergeupdates.effectiveDate \\\n",
    "WHEN NOT MATCHED then \\\n",
    "INSERT *\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4eb2ab3a-1688-4dbc-86af-1334b9a27716",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+-------------+----------+\n",
      "|primaryKey|             address|current|effectiveDate|   endDate|\n",
      "+----------+--------------------+-------+-------------+----------+\n",
      "|        11|  An updated address|   true|   2020-10-22|      null|\n",
      "|        11|A new customer ad...|  false|   2023-04-20|2020-10-22|\n",
      "|        41| A different address|   true|   2023-04-20|      null|\n",
      "|        58| Yet another address|   true|   2023-04-20|      null|\n",
      "|        99|A completely new ...|   true|   2020-10-22|      null|\n",
      "|        22|Some customer add...|   true|    2022-1-20|      null|\n",
      "|        33| A different address|   true|   2021-05-20|      null|\n",
      "+----------+--------------------+-------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM changing_addresses\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b816ba1-60cc-412a-a30c-fae180427835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 09:21:52 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
      "23/04/20 09:21:52 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another merge working\n",
    "spark.sql(\"MERGE INTO changing_addresses as original \\\n",
    "USING ( \\\n",
    "  SELECT some_address_updates.primaryKey as merge, some_address_updates.* \\\n",
    "  FROM some_address_updates \\\n",
    "  UNION ALL \\\n",
    "  SELECT null as merge, some_address_updates.* \\\n",
    "  FROM some_address_updates INNER JOIN \\\n",
    "  changing_addresses original on some_address_updates.primaryKey = original.primaryKey \\\n",
    "  WHERE original.current = true \\\n",
    ") mergeupdates \\\n",
    "ON original.primaryKey = mergeupdates.merge \\\n",
    "WHEN MATCHED and original.current = true THEN \\\n",
    "UPDATE SET current = false, endDate = mergeupdates.effectiveDate \\\n",
    "WHEN NOT MATCHED then \\\n",
    "INSERT *\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc0ca1ba-ca6b-45ef-95fe-fe624cce95ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+-------------+----------+\n",
      "|primaryKey|             address|current|effectiveDate|   endDate|\n",
      "+----------+--------------------+-------+-------------+----------+\n",
      "|        11|  An updated address|   true|   2020-10-22|      null|\n",
      "|        11|A new customer ad...|  false|   2023-04-20|2020-10-22|\n",
      "|        41| A different address|   true|   2023-04-20|      null|\n",
      "|        58| Yet another address|   true|   2023-04-20|      null|\n",
      "|        99|A completely new ...|   true|   2020-10-22|      null|\n",
      "|       101|Yet another custo...|   true|   2018-01-02|      null|\n",
      "|       102|    One more address|   true|   2018-01-01|      null|\n",
      "|        22|Some customer add...|   true|    2022-1-20|      null|\n",
      "|        33| A different address|   true|   2021-05-20|      null|\n",
      "+----------+--------------------+-------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM changing_addresses\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2ea4f-211c-4b73-9389-d3f16d600cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
